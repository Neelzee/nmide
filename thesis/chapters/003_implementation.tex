\chapter{Implementation} \label{cha:impl}

This chapter will focus on the implementation of the zero-core \gls*{ide}. In
section \ref{sec:stack}, we will mention technologies used, and why they were
chosen. In section \ref{sec:mod1} and \ref{sec:mod2}, we will discuss the
different iterations the application architecture had, and why they were subpar,
compared to section \ref{sec:moD3}, which is the implementation of the zero-core
\gls*{ide}. Section \ref{sec:testing} will explain the necessity of testing when
using such a modular design, and explore the ease of which functionality can be
tested in such a modular architecture. Section \ref{sec:modules} and
\ref{sec:module-installation}, will discuss the implementation of \gls{ide}
specific functionality, how modules are installed, and module development tools,
respectively.

\section{Tech stack} \label{sec:stack}

A module can extend an application at either compile time, or during runtime.
This could be achieved by using an interpreted language like JavaScript or
Python. The issue with using a dynamically typed language like Python or
JavaScript, is that it enhances the risk for runtime issues occurring, and when
dealing with scenarios like writing to files, or running long processes like
compiling a program, it is important to avoid such issues. So using a type safe
language, that can \textit{transform} runtime errors into compile time errors,
is preferred. Furthermore, to be able to support modules in foreign languages,
necessarily means that the core \gls*{ide} needs good \gls*{abi} support, and
therefore should be implemented in a low level language. But what does
\textit{low level} language mean? And what is an \gls*{abi}?


\subsection{Low level languages}

Programming languages has changed over time. In the beginning, a program was a
series of ones and zeros, representing instructions a computer should do. Since
then, we have moved several abstraction layers above what is commonly referred
as \textit{bare metal} programming. From writing in hexadecimal instead of
binary, to machine instructions, to more generic programming language, like C.
What was different with C, compared to writing direct machine instructions, was
that an external program, a compiler, could translate C code to machine
instructions specific to the computers' \gls*{cpu} architecture, this meant a
single program, written in C, could be compiled to many different computers. So,
at the time C came out, it was considered a \textit{high level} programming
language, because the language a developer was writing in, had a higher level of
abstraction.

Today this notion of \textit{low} and \textit{high} level languages has changed.
A \textit{low level} language is close to how a \gls*{cpu} \textit{thinks},
which has traditionally meant that C is a low level programming language, but
some~\cite{cNotLowLevel} argue that this is no longer the case. In any case, we
will use \textit{low level} to mean a programming languages like C, where direct
memory manipulations is a feature of the language.

\subsection{Application Binary Interface (ABI)}

An \gls*{abi} is an low-level interface, a kind of \gls*{api},
between two programs. Such as C program and its dynamic library dependencies.
The \gls*{abi} defines how data is laid out in memory, how functions are
invoked, and other machine level details. Both the C program and the dynamic
libraries must agree on the \gls*{abi}, otherwise misinterpreted data or invalid
function calls could lead to \gls*{ub}.

\paragraph{Undefined behavior} In programming \gls*{ub} occurs when a program
violates the language specification in a manner that is not defined by the
specification. This can be the results of \gls*{abi} mismatch, like if the
layout of a struct in memory differs between the parties, in a manner which
leads to breaking of type safety, or direct violations of the language rules,
like null pointer dereference. \gls*{ub} is dangerous because the compiler might
optimize the binary unpredictably or the program may behave arbitrary. It is
also a vector of attack for hackers.


\subsection{Rust}

Rust is a general purpose programming language, designed for, amongst other
things, type safety, memory safety, and concurrency. When programming in Rust,
the bugs common in other languages, like null pointers, buffer overflow and data
races are detected at compile time. Most of these are features of Rusts
ownership rules. These rules, enforced by the compiler, ensure that values are
safely dropped\footnote{Called \textit{freed} in Rust}, this ensures that all
variables referenced in Rust have a value, and can be safely evaluated. It works
by simply dropping values when they are out of scope.

The example in listing \ref{lst:ownership} showcases the ownership. The
\textbf{name} variable is declared, and used as an argument in the
\textbf{greeting} function. We cannot call the function again with
\textbf{name}, since at the end of \textbf{greeting}, before it returns,
\textbf{name} is dropped, since once we called \textbf{greeting}, the
\textbf{main} method no longer \textit{owned} \textbf{name}, as the ownership
was transferred to \textbf{greeting}. We could \textit{fix} this by changing the
argument type from \textit{name: String}, to \textit{name: \&String}\footnote{Since String is a dynamic heap string type, when we have a reference to it, it is equivalent to an \textit{\&str}, being a reference to an immutable sequence of bytes.}
and adding the borrow symbol to the argument in
the method invocation, as shown in listing \ref{lst:ownership-ref}.

\begin{code}[H]
  \lstinputlisting
    [ language=Rust
    , caption={
      Ownership example, this code snippet will not compile because we are
      violating the ownership rules. (Rust)
    }
    , label=lst:ownership
    ]{./code/rust-ownership.rs}
\end{code}

\begin{code}[H]
  \lstinputlisting
    [ language=Rust
    , caption={Ownership example with reference (Rust)}
    , label=lst:ownership-ref
    ]{./code/rust-ownership-ref.rs}
\end{code}

Now, the \textit{greeting} function is borrowing the variable, which means
restrictions are placed by the compiler, on what the function can do. It cannot
mutate the variable, which means that several different functions can read the
same data concurrently. We can also only mutate the variable if there are no one
borrowing it. This principle ensures the other mentioned features of the
language, including performance, can be guaranteed by the compiler.

Another Rust feature are so-called \textit{macros}. A macro is some code that is
evaluated and executed at compile time, that may change the source code. An
example of this, can be seen in listing \ref{lst:ownership}. The
\textbf{println!} is a macro invocation. \textbf{println!} is used so that the
developer does not have to format the expressions being used, this is handled by
the macro. The macro knows the type of the different variables passed, and
therefore how to print it to the terminal. Macros are helpful because redundant
work can be automated.

Furthermore, Rust has good cross-platform support, ensuring
we can write \gls*{os} agnostic code, and compile it to specific targets,
without much hassle. Since Rust is low-level, it has good bindings to C,
ensuring compatibility with future models, made in other languages, by use of
the Rust \gls*{abi}.

\subsubsection{Rust Application Binary Interface}

Rust's \gls*{abi} is not stable, as it is not supported by their semantic
versioning. This means even a bug fix in the compiler, could break the
\gls*{abi}. So if an application, written in Rust, is compiled in version 1.8.0,
if this application relies on a Rust library that is compiled in version 1.8.0,
everything is okay. But if the application is later recompiled with a compiler
in version 1.8.1, then \gls*{ub} could occur. One of the ways \gls*{ub} was
avoided, was using the \textit{abi\_stable} crate\footnote{\url{https://github.com/rodrimati1992/abi\_stable\_crates/}},
which enables \textit{safe} loading of external libraries.

With the abi\_stable crate, if the types in the \gls*{ide} change, either by
expansion or renaming or such, the crate would crash the application during
startup, because the existing module would have a different expectation of what
types existed, which again, could lead to \gls*{ub}. But, this due to the
implementation of a runtime module using the \textit{abi\_stable} crate, as one
could design a module to be expanded in the future, safely. This would mean we
could only add new functionality to the module, not change the old ones. But due
to the stability of the \gls*{api}, this was deemed unnecessary.

An interesting side effect of using the abi\_stable crate, is that the crate
\textit{forces} the types and functions to be implemented in a certain way, and
that way meant that they are closer to the C-\gls*{abi}. Which means integration
with foreign languages are easier.


\subsubsection{Module ABI}

Any programming language that has C bindings, can create a module, as that is
the bridge between the original, \textit{foreign} language, and Rust. We could
bridge the languages by simply creating a module which does the translation to
and from the different languages, either for each foreign module, or a singular
one, acting as a translator, translating the data flowing between the core and
foreign-modules as shown in figure \ref{fig:fm1} and \ref{fig:fm2} respectively.

\begin{figure}[H]
  \begin{subfigure}[h]{0.49\linewidth}
    \input{./figures/foreign-module-1}
    \caption{Foreign modules being invoked by a singular translation-module}
    \label{fig:fm1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[h]{0.49\linewidth}
    \input{./figures/foreign-module-2}
    \caption{Foreign modules being invoked by an individual translation-module}
    \label{fig:fm2}
  \end{subfigure}
  \caption{
    Two diagrams showing the different methods for implementing a translation
    module
  }
  \label{fig:fm}
\end{figure}

A Haskell module, for example, would require some translation module, either a
unique translator for each module, where the Rust module would start the
required Haskell runtime\footnote{https://ghc.gitlab.haskell.org/ghc/doc/users\_guide/runtime\_control.html},
when invoking the Haskell module, and exit before returning, avoiding \gls*{ub}.
The reason Haskell is of interest, is that the new compiler is written in
Haskell~\cite{wiig}, and as such, if one where to develop modules capable of
features like error reporting, go-to-definitions, and similar for the
\gls*{ide}, a translation module from Rust to Haskell could be necessary.


\subsection{Tauri}

Tauri\footnote{\url{https://tauri.app/}} is a framework for Rust, which enables
us to create a cross-platform application. Any frontend framework that compiles
to \gls*{html}, JavaScript and \gls*{css} can be used as the \gls*{gui}. Such a
\gls*{gui} is commonly referred to as a \textit{web view}. This framework also
adds support for invoking Rust methods in the frontend framework, and
vice-versa. This allows for support of JavaScript modules, without much fuzz.
Tauri archives with \gls*{ipc}, which allows for isolated processes to
communicate securely. For JavaScript to Rust, this is achieved with something
called \textit{Commands}, which acts as an abstraction on top of the \gls*{ipc},
which turns the invocation to a frontend-backend architecture.

Tauri also comes built-in with a lot of utilities, for when it comes to
application development, like bundling the application for different OSes,
abstracting away file system operation, making them independent of OSes, general
application security, and creation of dialogs. These dialogs allow for, amongst
other things, a user to chose file paths. A user can be prompted with a file
selection, click on a file, and click open, and this will be returned as a path
in Tauri, which we can use to manipulate the file.


\subsection{Security}

This framework gives a lot of security which is needed in an
application which runs third party code. An example of this, would be the
so-called \textit{isolation pattern} that Tauri supports. Since we allow for
evaluation of JavaScript code in the \gls*{ide}, we allow for third-party-code to
access all of Tauris \gls*{api}. This \gls*{api} is quite powerful, allowing for
features such as access to the system shell, the file system, etc.

Being powerful, it is dangerous to expose this to third-parties, but we can
intercept and modify all Tauri \gls*{api} calls sent from the JavaScript side,
before they reach Tauri. We can therefore, depending on the perceived threat or
sensitivity of a Tauri \gls*{api} call, choose to disregard the invocation of the
\gls*{api}. We could also choose to disregard \textit{all} invocations that comes
from the JavaScript side, effectively only allowing Tauri \gls*{api} invocations
through the \gls*{ide} itself, meaning only Tauri functionality exposed by the
\gls*{ide} itself are callable.

This of course has \textit{some} performance implications, as we now have some overhead on
each Tauri \gls*{api} invocation, but this is negligible\footnotemark{}.

\footnotetext{According to Tauri themselves \url{https://v2.tauri.app/concept/inter-process-communication/isolation/\#performance-implications}}


\subsubsection{TypeScript}

TypeScript offers a lot of features over JavaScript, amongst them being able to
\textit{type} functions, ensuring null/undefined-safety, at least in our own
project. Making TypeScript an ideal candidate for our frontend implementation.
Furthermore, by using crates like \textit{ts-rs}\footnote{\url{https://github.com/Aleph-Alpha/ts-rs}},
Rust types can be annotated with attribute macros, which create a one-to-one
mapping between the Rust type, and the serialized JSON object, to be used in
TypeScript, allowing for even more type safety, and ensuring that the types used
in the \gls*{ide} only have to be defined one place.

Since the \gls*{ui} is managed by JavaScript, it means any JavaScript library
can be used in our \gls*{ide}. Allowing for any JavaScript library to be used,
enabled a low development time of \gls*{ui} components. Since \gls*{ui} is a
well explored field, there exists a lot of existing \gls*{ui} libraries,
especially for JavaScript. Npm\footnote{https://www.npmjs.com/}, for example, is
a package registry for JavaScript. It contains around $3.4$ million libraries,
all of which are usable in this architecture. If the functionality that these
libraries are useful for the application, is another question. This
functionality allows for quick development time for modules, which means
features that are standard in \gls*{ide} can be quickly and easily added. But
this also means we have to validate modules written in JavaScript, as we can
only guarantee the safety of a module during runtime.


\subsubsection{Module validation}

Running third-party-code can dangerous. If this code is not validated or does
not come from a trusted source, it could be an attack vector. Luckily, Tauri
does some of this work for us, allowing us to analyze all module to core
communication, but even if we have validated that a module comes from a source
the user trusts, we still need to ensure the module is implemented correctly.

The Rust compiler can ensure that the Rust modules are valid during compile
time, for runtime this is a bit trickier. But for JavaScript modules, which the
\gls*{ide} supports out-of-the-box, this is a bigger issue. This lead to the
development of two systems. \gls*{rsms} and \gls*{jsms}.

It was necessary to distinguish the different module systems, due to the way
they would be loaded and invoked by the core application. Since the core is
written in Rust, the \gls*{rsms} doesn't have to do any validation or
translation when communicating with compile time modules. With runtime modules
this also ended up being trivial, with the abi\_stable crate, but will be
discussed more in depth later.

In the \gls*{jsms}, managing of modules can lead to exceptions being thrown.
Since third party code is being run, nothing can be trusted. All module
invocations and outputs needs to be sanitized before it can be used in the core
application. This is achieved by wrapping all invocations in a
\textit{try-catch}, and using the \textit{io-ts}\footnote{\url{https://github.com/gcanti/io-ts}}
library to decode types during runtime.

This enables us to safely invoke modules, as we can translate all computations
into a product type, where it is either a success, giving us the wanted
computation from the module, or an error. But even with types, we cannot verify
functions. Since during runtime, we are in the JavaScript environment, we can
only validate if something is a function, using the
\lstinline[language=JavaScript]{typeof} operator. It is possible to do
\textit{some} verification on functions in JavaScript, but this can only tell us
that it is a function. Nothing about the typing of the function can be
ascertained at runtime, without explicitly invoking the function.


\section{Module v1} \label{sec:mod1}

We did not attempt at first, to create a zero-core application; this was a
\textit{natural} conclusion to the existing problem. The first attempt was a
simple generic \gls*{ide}, in which the module architecture was a concern from
day one of development. The general plan was this:

\begin{enumerate}
  \item Create an \gls*{ide}
  \item Extend the \gls*{ide}, to allow for a module architecture
  \item Modules call the application using some \gls*{dsl}
\end{enumerate}

Since any JavaScript frontend framework could be used, React was chosen, one of
the reason for this choice was due to its popularity, which again, would speed
up the development time of the application, atleast the \gls*{ui}. But also due
to the way React renders the \gls*{html}. Between two different re-renders of
the application, React can check the difference between the \gls*{vdom}, which
is React's representation of the \gls*{dom}. It then only changes what is needed
in the \gls*{dom}, instead of re-creating the entire \gls*{dom}, which makes the
render time quick.

This was the more straight forward way to work, because as we could model it of
existing \gls*{ide}s, like VS Code or Eclipse. Another advantage is that when
implementing the application, one necessarily gets a better understand of how
eventual modules should extend the application.

When we came to the final step, it immediately became clear that naively
creating a \gls*{dsl} for functionality was impractical. We started with the
idea of, try to implement a feature of the \gls*{ide}, so extend the \gls*{dsl},
but this approach did unfortunately not lead to a truly modular application.
Similar issues to existing \gls*{ide}s, how does one allow for
\textit{everything}? Furthermore, anything created this way, would be subpar to
existing software, which would lead to the next maintainer having to fix the
core application. This in turn, would add a lot of complexity, which the
maintainers would have to deal with.


\section{Module v2} \label{sec:mod2}

\begin{itemize}
  \item Everything is a module
\end{itemize}

Instead of developing features that make up an \gls*{ide}, and attempting to
ensure it is implemented in such a manner that it can be modified in the future,
make everything modular. The only thing the \gls*{ide} can do, is to manage
modules. All features, from the file explorer to the text editor, everything is
a module that can be enabled or disabled. A zero-core, modular architecture.

\subsection{The Elm architecture}

An inspiration for the new module architecture is Elm-Lang~\cite{elmLang}. Elm
is a functional language, aimed at frontend web development, but its
architecture is quite interesting. As one can see in figure
\ref{fig:elmArchitecture}, the Elm-runtime translates the Elm code into
\gls*{dom} manipulations, and translates \gls*{dom} events into \textit{Msg}
which is handled by the Elm code. This was the inspiration for the new module
architecture. A module is managed by the runtime, which is the \textit{core}
application. But with some inspiration from \gls*{mvc}, where instead of the
module keeping its own state, this is again managed by the core, allowing for
multiple modules to read and react to states updated by other modules, allowing
for more interactivity between modules, and therefore being more modular.

\begin{figure}[H]
  \centering
  \input{./figures/elm-architecture}
  \caption{Elm Architecture (Figure adapted from~\cite{elmFig})}
  \label{fig:elmArchitecture}
\end{figure}

\subsection{Module architecture}

In this architecture, the Elm-code is a module, while the runtime system is the
core itself. The core invokes all modules, all of which, should have these three
functions, \lstinline{init}, \lstinline{update}, and \lstinline{view}.

\paragraph{Init} Returns a collection of key-value-pairs, which represent
the state of the core.

\paragraph{Update} Returns a collection of key-value-pairs, which
overwrite existing key-value-pairs in the state, or are appended to the state.
Invoked every time a \textit{Msg} is sent.

\paragraph{View} Returns a collection which represents \gls*{html},
which is rendered by the core.

A module is initialized by invoking the \textbf{init} method, which returns a
state. This can be seen in figure \ref{fig:moduleInit}. After the state
initialization, the modules' \textbf{view} method is invoked, which initializes
the \gls*{ui} for the user, which can be seen in figure \ref{fig:moduleInitView}.

\begin{figure}[H]
  \centering
  \input{./figures/plugin-init}
  \caption{Module state initialization stage}
  \label{fig:moduleInit}
\end{figure}

\begin{figure}[H]
  \centering
  \input{./figures/plugin-init-view}
  \caption{Module view initialization stage}
  \label{fig:moduleInitView}
\end{figure}

Since the \gls*{ide} is written in both TypeScript and Rust, a method of encoding
type information when crossing between the TypeScript and Rust environment was
needed. It was achieved by simply typing \gls*{json} objects, so while the state
could be represented as any \gls*{json} object, it was instead represented as
nested \gls*{json} objects, where, all values, except \textbf{null}, where
encoded as an object with one field, being the type of the object, and then the
value. So an int would be \textbf{\{ int: 0 \}}.

The reason for representing a JSON object as key-value pairs, is that this could
be easily translated to a Rust representation of the same type, using the
\textit{Serde} crate. This allows for creating Rust structs which represents
JSON objects, and creates an automatic encoder/decoder between Rust and
\gls*{json}. Using the ts\_rs crate, we could also automatically create the
TypeScript type that represents the automatically encoded/decoded \gls*{json}.
This ensures a good cooperation between the \textit{frontend} and
\textit{backend}.

\subsection{Module lifecycle}

The general idea was that for each possible \gls*{dom}-event, there would exist a
way to send a Msg. Each Msg contains a Msg name, and some value, which enabled
pattern matching on Msg, similar to Elm, for modules, so each module could
choose to act on a Msg or not. So, after the initialization of the \gls*{ide},
any time the user interacted with the \gls*{gui} the modules would react to the
Msg. The trivial plugin, would simply return an empty state on \textbf{init} and
\textbf{update}, while on \textbf{view}, it would return a \textit{frag}
element, which is a React element that evaluates to no \gls*{dom} change.

In listing \ref{lst:pluginCounterExample}, an example of a counter module can be
seen. This module initializes a state, containing the field \textbf{"counter"},
with the value \textbf{VInt 0}. The module is initialized, by invoking the
function on line two.

\begin{center}
  \lstinputlisting
    [ language=Haskell
    , numbers=left
    , numberstyle=\tiny\color{gray}
    , caption={Counter Module (Haskell)}
    , label=lst:pluginCounterExample]{./code/plugin-counter-example.hs}
\end{center}

The \textit{update} function the module exposes, on line five, pattern matches
on a \say{counter} msg, with a \textbf{VInt i} value. If the given msg matches
this, then the module adds to the \say{counter}-field, the value from the msg,
which is $1$. The module is invoked for all msgs being sent, so it is up to the
module developer to ensure the match on the correct msg. We can see that we
have a \textit{catch-all} for all Msg that we do not care about, on line ten.
Here we just return the input state, with no changes.

Finally, the \textit{view} function, on line 12, renders a button, which when pushed by a
user, sends the \textit{counter-msg}, which we pattern match. This is done by
adding an \textit{eventListener}, to the \gls*{html} button, where we emit the
passed msg on every click.


\subsubsection{Module purity}

One important thing in this architecture, is the pureness of module. We use this
term, \textit{pureness}, to mean side effects \textit{outside} the \gls*{ide}s
\gls*{api}. In principle, this means two related things.

Firstly, given the same input, we get the same output. Of course, this is
possible due to the entire \gls*{ide} being part of the input, but we mean that
a module should not have an internal state, outside the \gls*{ide}.

Secondly, specifically for JavaScript modules, modifications of the \gls*{ui},
should only occur through the \gls*{ide}.

The reason for the requirement of pureness twofold. It allows for the
possibility of the core to be optimized in the future, as modules which do not
react to a certain msg-state combination, can be noticed, and ensure modules are
not unnecessarily invoked. It also lowers the complexity for module developers,
as it is easier to reason about modules if \textit{all} they do is read or write
to some state.


\subsection{Module v2 cons}

While this approach was better than the first iteration, we still encountered
issues, some of which could not be solved with this approach.

\paragraph{Not modular} This setup is also not really modular, as a single
module cannot invoke another module without being impure. The only way to
invoke/trigger another module, is to throw a msg, which would trigger an update
$\to$ view $\to$ cycle. So a module cannot \textit{listen} for a single message,
all modules are triggered by the same msg, and handled accordingly,
synchronously. This is clearer if we visualize the architecture. In the figure
\ref{fig:modulev2}, we can see a diagram of the architecture. Note the arrows,
signaling how they interact with each other.

\begin{figure}[H]
  \centering
  \input{./figures/module-v2-architecture}
  \caption{
    Module v2 architecture diagram, showing how all modules can only interact
    with the core.
  }
  \label{fig:modulev2}
\end{figure}

They all go through the core, there is no way for $module_0$ to invoke just
$module_1$.

\paragraph{Synchronous module invocation} If a msg triggers a computational
heavy method, the \gls*{ide} will \textit{hang}, and act \textit{sluggish} until
the computation has finished. This would also affect \textit{all} modules,
since they are invoked in order, regardless of if they actually change the
state or view.

\paragraph{Ever-growing state} There was no way to remove a field on the state,
the state is appending/overwriting -only, which was a side effect of the
coalescing of states, as we looked at the differences between the previous
state, and the new state, and if the new state did not have a field that the
previous one has, we kept it. If they had the same field, the new state
overwrote the old one. So since we only did tree comparisons, there was no way
to encode removal of a field.

\subsubsection{State collision} \label{sec:collision}

A state collision occurs when two or more modules updates the same field, during
the same update-cycle. This issue also occurs when folding two states. After any
update-cycle, we were left with a list of states, which needed to be coalesced
into a singular one. There are several different ways to correct a collision
between two states:

\begin{enumerate}
  \item If the states are of same type:
    \begin{enumerate}
      \item If the value from one of the colliders are unchanged from the previous state:
        \begin{enumerate}
          \item Keep the new value OR Keep the old value
        \end{enumerate}
      \item Else
        \begin{enumerate}
          \item Apply the types' semigroup operator to the fields.
        \end{enumerate}
    \end{enumerate}
  \item Else
    \begin{enumerate}
      \item If the value from one of the colliders are unchanged from the previous state:
        \begin{enumerate}
          \item Keep the new value OR Keep the old value
        \end{enumerate}
      \item Else
        \begin{enumerate}
          \item Keep the left-hand side value OR Keep the right-hand side value
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

Since the states are ordered by the name of the module they come from, we
have a consistent ordering of left-hand side and right-hand side. Due to the
fact that module invocation is synchronous, and ordered. If the same modules
give a collision on the same input\footnote{Given that all modules are pure}, the
resulting state will be the same every time.

The problem is that applying some semigroup operator on the values could be an
unwanted way to resolve collisions. It is clear for some of the types what the
operation is, like for the number types, it could be addition, but that would be
a choice we are taking away from the module developer, and not a behavior they
could change.

Therefore, the standard way will be to log the collision, and then drop both
states. Even if two states have $A$ and $B$ amount of fields, and just one
collision, we would drop $A + B$ amount of fields.

This problem of resolving state collision only occurs because each module
returns a subtree of the state. We then have to analyze the new tree coalesced
from all modules, to figure out if there occurs any collision. And then
notifying the module developer of which field this collision occurred on, and
which modules tried to modify that field.


\section{Module v3} \label{sec:moD3}

To solve the issues with the previous iteration, we decided to add another
requirement, and the structure of a module:

\begin{itemize}
  \item Everything is a module
  \item Modules can \textit{invoke} modules
\end{itemize}

A module now only exposes two functions:

\paragraph{Init} Returns nothing

\paragraph{Handler} Returns nothing

But given how neither of these functions return anything, how can modules affect
the core? A module can \textit{send} a set of instructions, which we call core
modifications, to modify the core. These modifications are \textit{sent} using
an \gls*{mpsc} channel, which enables communication between two different
threads, allowing for concurrent core modifications.

In the previous architecture version, each module directly changed the state,
which caused issues. Instead, each modification a module does, \textit{acts}, as
a direct modification, but is in fact, translated to a \gls*{dsl} which can be
analyzed for possible collisions. This was discovered to be a need, as in the
new version, the \gls*{ui} was also restructured, to allow for less
re-rendering, and this restructuring, made it clear that changing the state, or
changing the \gls*{ui} is just tree manipulations, which will be discussed more
later.


\subsection{Zero-core architecture and microservice architecture}

The new plan came with a change of viewpoint. Think of
\textit{everything being a module}, this pushed for a modularization between the
then tightly coupled parts, the \textit{frontend} and \textit{backend}. As
mentioned, having two different languages could allow for easier support of
modules written in different programming languages, but for this to work in an
optimal way, both the \textit{frontend} and \textit{backend} should be loosely
coupled.

Furthermore, modules themselves should also be loosely coupled, where they
invoke or are invoked by other modules. This is an equivalent architecture to
microservices. In the figures \ref{sfig:mic} and \ref{sfig:mod}, we can see the
equivalence side-by-side.

\begin{figure}[H]
  \begin{subfigure}[h]{0.49\linewidth}
    \input{./figures/microservice}
    \caption{Basic diagram of a microservice architecture}
    \label{sfig:mic}
  \end{subfigure}
  \hfill
  \begin{subfigure}[h]{0.49\linewidth}
    \input{./figures/moduleservice}
    \caption{
      Diagram showing communication between modules.
    }
    \label{sfig:mod}
  \end{subfigure}
  \caption{
    Two diagrams showing the similarities between a microservice and zero-core
    module architecture
  }
  \label{fig:comp}
\end{figure}


\subsection{Vanilla TypeScript}

Instead of using React as the frontend framework, TypeScript was chosen, which
simplified the integration between the backend and frontend, as the complexity
of React's state management could be avoided, along with React's hydration.
Given the rendering was now more \textit{hands-on}, the core could expose a lot
of the functionality for rendering, which modules could change. This would
increase the difference between the \gls*{jsms} and \gls*{rsms}, as the backend
was not privy to this \gls*{api}, but this was not seen as an issue, as this
\gls*{api} would turn module non-pure. Reacts state management with
\textit{useEffect} and \textit{useState} where needed to handle efficient
re-rendering of the \gls{dom}, but since we were working directly on the
\gls{dom}, this state management was just in the way. So moving to just
TypeScript meant that it would enable easier to develop the \gls*{jsms}.


\subsection{Core modifications}

Learning from the issues outlined in section \ref{sec:collision}, instead of a
module returning the new core, it will rather return a set of instruction on
\textit{how} the core is to be modified, resulting in what the module developer
wants the core to be. The reason for turning it around in this manner, is that,
the new architectural change also came with a change on how the \gls*{ui} is
modeled, as it is now up to the core to figure out an inexpensive way to do
rendering. Since the core has \gls*{ui}-structure which is a representation of
what the \gls*{dom} should be, it can be treated as a Virtual-\gls*{dom}, similar
as to how React does it. This also means that there could be a collision on
\gls*{ui}-change, as well as on a state-change. Instead of solving the equivalent
problems twice, it was decided to try to treat the issues with collisions in
state and \gls*{ui} as the same issue; it is some form of tree-manipulation. We
could therefore reduce the amount of needed methods on the module instance, to
two. One for initializing the state, and one for handling events. In
\ref{lst:cm}, we have a \textbf{CoreModification} type, which has two fields,
one for the state, and one for the \gls*{ui}.

\begin{center}
  \lstinputlisting
    [ language=Rust
    , caption={
      Core Modification struct, it contains two fields, state and ui. Using a
      instruction type, parameterized on value, we represent state. The ui field
      contains a type alias for a triple, containing the instruction type, but
      parameterized on a \textit{html}, \textit{attribute} and string variants.
      (Rust)
    }
    , label=lst:cm
    , firstline=12
    , lastline=15
    ]{./libs/rust/core-std-lib/src/core_modification/mod.rs}
\end{center}


\subsection{Instruction based tree manipulation}

This restructure changes the way the view is rendered. Instead of the view being
re-rendered for each state-update, the view, or \gls*{ui}-hierarchy, is only
modified by modules. This modification is similar to the earlier state
modification, so a unified algorithm to solve this can be used. If there is an
easy way to translate a \gls*{ui} modification to a state modification, and back
again. To solve this, instead of having a module return the actual
modifications, meaning, the updated core, a module returns a set of instructions
of what to do with the core.

\begin{code}[H]
  \lstinputlisting
   [ language=Rust
   , caption={Instruction (Rust)}
   , label=lst:inst
   , firstline=6
   , lastline=16
   ]{./libs/rust/core-std-lib/src/instruction/inst.rs}
\end{code}

In the listing \ref{lst:inst} we can see the Rust implementation of our
instructions. Given how this is a recursive data-type, it is a safe assumption
to make that this forms some kind of algebraic structure. As we have a kind of
set, where the elements are variants of our instructions. This algebraic
structure of the instruction-set comes more clear, if we remove the syntax, and
just focus on the semantics.

We have three instructions, $NoOp$, $Add_T$, and $Rem_T$. Those are our variants
in our set, where the subscript $T$ indicates which type our instruction are
parameterized by. If $Then$ is our binary operation, combining our instructions,
it is clear that our operator makes a tree structure out of our instructions. We
want this to be associative, which means the equation \ref{eq:instrAssoc} should
hold.

\begin{equation} \label{eq:instrAssoc}
  Then'(Then(Rem_T, Add_T), Add_T') = Then'(Rem_T, Then(Add_T, Add_T'))
\end{equation}

In the figure \ref{fig:instrTree}, we can see diagrams of two trees,
in figures \ref{sfig:instrTree1} and \ref{sfig:instrTree2}, which are a
representation of the different trees from the left and right side of the
equation \ref{eq:instrAssoc}.

\begin{figure}[H]
  \begin{subfigure}[h]{0.49\linewidth}
    \centering
    \input{./figures/instr-tree-1}
    \caption{
      Diagram of the resulting tree from $Then'(Then(Add_T, Add_T'), Rem_T)$
    }
    \label{sfig:instrTree1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[h]{0.49\linewidth}
    \centering
    \input{./figures/instr-tree-2}
    \caption{
      Diagram of the resulting tree from $Then'(Add_T, Then(Add_T', Rem_T))$
    }
    \label{sfig:instrTree2}
  \end{subfigure}
  \caption{
    Diagram of the tress from right and left sides of the equation
    \ref{eq:instrAssoc}.
  }
  \label{fig:instrTree}
\end{figure}

It is clear that these are two different trees, but when we evaluate the tree,
we can see that the order of operations are the same. If we evaluate right side
first, we get this order: $Add_T, Add_T', Rem_T$, and if we evaluate the left
side first, we get this order: $Rem_T, Add_T', Add_T$. They are different
orders, but regardless of which order we choose, both sides are equal.
Furthermore, it is clear that $NoOp$ is our neutral element, giving us a monoid,
as defined in \ref{def:monoid}. But we also have \textit{idempotency} on our
instructions, if we apply the same $Add_T$, or the same $Rem_T$, several times,
it is the same as applying it just once.

It is very useful to think about what kinds of properties our structures have,
as with this knowledge, we utilize existing theories to improve our software.
Since this instruction structure is used for \gls*{ui} rendering, it is useful
to minimize the amount of needless renders, which we can do by applying the idea
that our instruction set is an algebraic structure with properties.

But these properties are not possible to encode in Rusts type system, but when
implementing $combine$, we can map the variants along with the specific fields
being added ($Add$), modified ($Mod$), or removed ($Rem$), to get a more
optimized instruction set. If we are modifying a value on field $foobar$, but in
the same instruction set, remove it, then the modifying instruction is an
$NoOp$. This optimization can be found in appendix \ref{app:a}. The result is
that we can utilize algebraic structures to reduce the amount of data we send
to the user.


\subsubsection{Instruction utility functions}

Like writing direct binary to develop a program, writing \textbf{instruction}s to
change the core is quite abstract for most developers so to facilitate development
of modules, a helper class was created, which \textit{translates} modifications
to instructions. As shown in listing \ref{lst:ui-builder} and
\ref{lst:state-builder}, a module developer simply invokes different methods on
the builder, eventually building a \textbf{CoreModification}, to be sent.

\begin{center}
  \lstinputlisting
   [ language=Rust
   , caption={
     UI Builder (Rust) showcasing how to add an empty \gls*{html} div element to
     the root \gls*{html} element.
   }
   , label=lst:ui-builder
   ]{./code/module-ui-builder.rs}
\end{center}

\begin{center}
  \lstinputlisting
   [ language=Rust
   , caption={
     State Builder (Rust) showcasing how to add a \textit{count} field to the
     state, also showcasing how Rust can infer that the i32 type $0$ is
     a \textbf{Value::Int} type.
   }
   , label=lst:state-builder
   ]{./code/module-state-builder.rs}
\end{center}

This allows for an ergonomic way for module developer to create modifications on
the core, without having to understand the syntax of the
\textbf{Instruction}-set.


\subsection{Backend agnostic frontend}

Since we are using the framework Tauri to implement the \gls*{ide}, the \gls*{ide}
is split to two, loosely coupled parts. The \textit{frontend} and
\textit{backend}. The frontend acts as a thin wrapper around the core \gls*{api},
enabling different \textit{runtimes} to handle module management, while the
frontend waits for events, and renders the \gls*{gui}. This structure allows for
future maintainers of the \gls*{ide} to be able to \textit{trivially} switch
runtime, if they wanted to use some other language to implement the runtime
system in, like PureScript, Gleam or Haskell, all of which can target
JavaScript, then they could. Indeed, this is ingrained in the base library, as
everything that is Tauri dependent, is wrapped in a trait, so that external
consumer of the base library can implement the necessary functionality given by
Tauri. An example of this, would be to implement the \gls*{ide} as a web
application. Ignoring, for a moment, the \gls*{rsms}, all modules exist on the
client side, so with minor tweaking, the application can be served as a web
app. Some more modifications and workarounds are needed, to support the
\gls*{rsms}, as the modules would exist as a singular instance across multiple
different users, but if the modules are \textit{pure}, this would not matter.
Then the only issue is to ensure consistency between the state and \gls*{ui},
which would be stored on the client side.

\subsection{Making the core evaluate modifications asynchronously}

Due to Rust first class focus on concurrency, it was trivial to make the core
modifications run asynchronously. In previous iterations, the core evaluated
one event at a time, waiting until all modules had finished their computations,
before emulating the change and allowing for the next event to be evaluated. But
this caused a noticeable \textit{lag} if an event was long.

This was solved by changing the core modification evaluation from a simple
method to be invoked, to an \gls*{mpsc} channel system. Using \textit{tokio}\footnote{\url{https://tokio.rs/}},
a Rust crate for asynchronous development, a channel for core modifications was
created, and instead of the core collecting all modifications, each module is
invoked and \textit{awaited} for in a separate thread, where in each module, if
they have a core modification, sends the modification to the core channel, which
works on a first come, first server basis. Here the core can evaluate the
changes, also on a separate thread.

This restructure was trivial to add, as due to Rusts borrow-checker, we know
that there are no race conditions and other similar concurrency issues in our
application.


\section{Testing} \label{sec:testing}

A zero-core \gls*{ide} is equivalent to a microservice architecture, so we can
use the similar lessons learned in microservice architecture, when developing
ours. In a microservice architecture testing is important to ensure changes in
one service does not inadvertently affect another. This is commonly achieved by
using \textit{pipelines}, a part of the \gls*{cicd} process, where we run
several \textit{jobs} whenever we make a change to our microservices.

In our case, with modules, if we are bundling different modules together, and
serving that as an \gls*{ide}, we want to ensure that a change to a module does
not negatively affect the other. This is where \textit{pipeline jobs} come in,
as each \textit{job} test some part of our \gls*{ide}. While it is cheaper to
\textit{spin} up an instance of the \gls*{ide} in a pipeline, than an
application dedicated to serve millions of users, we still want to avoid doing
this unnecessarily. This is why we split up our testing into different stages.


\subsection{Mocking}

Due to the \textit{pureness} of modules, mocking can be achieved easily, and
therefore, modules can be tested alone, which is good, because testing a
singular module is inexpensive. There are several ways to do this mocking in our
architectural setup. For both the \gls*{jsms} and \gls*{rsms}, there are
\textit{mock-cores}, which can mock the expected functionality of the core
instance, which we can extend to evaluate actual modifications, ensuring we can
assert that some state or \gls*{ui} change has occurred after an event has been
sent.


\subsection{Unit testing}

A module developer should create unit tests for their module. This can easily be
done, and tested many times, due to the light-weightiness of a module. This,
together with mocking, ensures we can test our modules, as if they were in a
\gls*{ide}. Which means we can ensure changes made to a module is non-breaking.

This also applies to maintainers of the \gls*{ide}, as maintaining the core
functionality and \gls*{api} of our library, means documenting possible breaking
changes, which unit tests can help ensure.

Furthermore, we also need testing of our core libraries. Since we are aiming to
support many different languages; to be language agnostic; we need different core
libraries providing utility functions that are widely used when developing
modules for our \gls{ide}. An example of this, could be the builder pattern we
use when creating the core modifications, as shown in listing \ref{lst:cm}.

To ensure these unit tests are uniform, and cover the same edge-cases, we have
designed test data, that the different libraries use when testing. For instance,
when developing with foreign languages, the data sent between the \gls{ide} and
modules, or in between modules, is in the \gls{json} format. But in a Rust
module, this would be deserialized into a Rust equivalent struct, before being
serialized back to \gls{json} when sent out from the module.

Similarly, in JavaScript, we have to ensure that we can actually handle the data
the \gls{ide} sends\footnote{Since the types are specified on the backend}.
Using the same test input, we can verify that both the Rust and JavaScript
library correctly handle the serialization and serialization of the data. Since
much of the base logic exists in the backend, this is where the brunt of the unit
tests exist.


\subsection{Library testing}

We also need to ensure that the libraries we serve to our module developers
are correct. The easiest way for a maintainer to achieve this, is to test in
production; letting module developers report issues. This is not a good
experience for module developers. But the second-easiest way is to create unit
tests that ensure edge cases are handled correctly. We have designed test data
for serializing and deserializing data between the different languages supported
by the \gls*{ide}, ensuring that any library developed can be verified to work
correctly. The test data is modular, meaning we can easily create new edge
cases. In the picture \ref{pic:libTest}, we can see that there are $135$
different test cases for the JavaScript library. In picture \ref{pic:libRsTest},
we can see that there are $5$ serializations tests, which is because we cannot
create a parameterized test from a file, like we can with the JavaScript testing
framework, Vitest\footnote{\url{https://vitest.dev/}}. But they both use the
same test data as input.

\begin{figure}[H]
  \begin{subfigure}[h]{0.49\linewidth}
    \centering
    \includegraphics[width=0.75\textwidth]{libtest.png}
    \caption{
      Picture showing the output of running the JavaScript library with the test
      data as input.
    }
    \label{pic:libTest}
  \end{subfigure}
  \hfill
  \begin{subfigure}[h]{0.49\linewidth}
    \centering
    \includegraphics[width=0.75\textwidth]{librstest.png}
    \caption{
      Picture showing the output of running the Rust library with the test
      data as input.
    }
    \label{pic:libRsTest}
  \end{subfigure}
\end{figure}


\subsubsection{Documentation testing}

In Rust any function annotated with a doc string, can contain code examples. If
these code examples are written as Rust code, and use assert statements, then
this code is run, during testing, as if it was an actual test. Meaning the
saying \textit{code is documentation}, can be \textit{documentation is code} in
Rust. In picture \ref{pic:libDocTest}, we can see some \textit{doc-tests} being
run on the library for runtime Rust modules.

\begin{figure}
  \centering
  \includegraphics[width=0.50\textwidth]{libdoctest.png}
  \caption{
    Picture showing the output of running the JavaScript library with the test
    data as input.
  }
  \label{pic:libTest}
\end{figure}




\subsubsection{UI testing}

We can also combine this with existing testing libraries, like Playwright\footnote{\url{https://playwright.dev/}},
which can enable us to create tests specifically for \gls*{ui} behavior. In the
case of Playwright, our \gls*{ui} testing is dependent on that our
\textit{mock-core} has the necessary functionality to transform the
\textbf{Html} type we have implemented, to actual \gls*{html}, which can be
rendered on a webpage, or \textit{headless}, in Playwrights case, so that
Playwright can assert the state of our \gls*{dom}.


\subsection{Module family testing}

If a module changes some feature, let's say in the editor functionality, the
module family tree encompassing this functionality needs to be tested, to ensure
nothing breaks. This means creating tests that use all the modules in a family,
and asserting that the state and \gls*{ui} behave as expected. In the case of
the editor functionality, that after the event \textit{open-file} is sent with
a path to some file, that there exists a \textit{textarea}-\gls*{html}-element in
the \gls*{dom}, with the same contents as the file.

\subsubsection{Contract testing}

As a module developer, on is designing some kind of \gls*{api}, but the developer
has no say in how a consumer of the \gls*{api} consumes it. In a microservice
architecture, the common way to work around this, is to version control the
\gls*{api} by prefixing \textit{v*} in front of all endpoints in the \gls*{api},
where star, (*), is the version of the \gls*{api}. This way, the \gls*{api}
designer can develop new \gls*{api}s, without worrying about breaking
functionality that consumers of the \gls*{api} depend on. This, however, usually
means having to maintain equivalent \gls*{api}s in parallel, until one decides
to deprecate an older less used version, forcing consumers to move on to the
newer version of the \gls*{api}.

Instead of relying on such a versioning system, module developers could use
\textit{contract testing}.

\paragraph{Contract testing} Imagine some \gls*{api}, and several consumers.
The \gls*{api} developer is serving some data, in this case an integer number,
which all the consumers use. One day, the developer finds out that using
integers is not optimal, and want to move on to using floating point numbers
instead. Changing the \gls*{api} outright could bring issues, as the consumers
might rely on the \gls*{api} being an integer, instead of a float. But the
change is needed, or wanted, at least. In this scenario, it is \textit{easy} to
inform all the consumers of the \gls*{api}, but if the consumer count increases
tenfold, this is more difficult. A notice can still be sent, but it is not
feasible to ensure all consumers commit time to change their ways. Contract
testing ensures that, if a change like this occurs, the maintainer of the
\gls*{api} is notified by which consumer this change breaks.

The issue is to create these contracts. Using frameworks like Pact\footnote{\url{https://docs.pact.io/}},
a developer creates a \gls*{dsl} test, where they describe how the provider or
consumer reacts to certain interactions. But since everything is a module, we
can automate this.

\subsection{Automating contract testing}

This process could be partially automated, as all modules have to register the
event they want to handle. Furthermore, all events thrown are also explicitly
done through the core instance, meaning a \textit{test-core} could be created,
which registers which event is thrown from what module, and all dependencies
between modules can be noted. This has been partially achieved. By loading all
specified modules, we can note their dependencies, by looking at the different
consumer and providers.

A module has two different states, initialization, and handling. During
initialization, a module cant have any dependency on other Modules, but it can
register for an event, meaning the module will be invoked once the specified
event is triggered, or it can throw an event. Registration for an event can
always be directly analyzed, as it happens on the core instance. The way a
registration occurs, is that a module supplies a string for a specific event
name to trigger on, meaning after the first initialization, the consumer graph
looks like this: $module \to event$.

To find the providers, we can initialize a module, and see what possible
events, if any are thrown. If an event is thrown, then that module is a
provider of that event. The mapping between event and module is not unique,
as several modules can provide the same event. Another way a module can
provide an event, is through user interaction. A module can create some
\gls*{ui}, which a user can interact with, which would trigger an event. The
\gls*{ui} can be analyzed for such triggers, and a mapping between that event and
module would be created.

During a handling of a triggered event, a module could register for new
events, trigger another event, or do nothing. To find more dependencies, a
module is triggered with the events it subscribed to, and analyzed for new
registrations or triggers. If the list all subscribed events have been
exhausted, and no new registration or trigger has occurred, that module is
considered \textit{exhausted}, and won't be analyzed anymore.

Since modules are asynchronous, and can possibly spawn their own threads, there
is a possibility for deadlocks to occur. There is no way to avoid this, without
restricting module developers, which is not wanted. Therefore, it is up to
the user of the module dependency graphing tool to avoid this, by supplying a
timeout value, ensuring that if, after some time $N$, the analyzation of a
module is still occurring, the analyzation is killed.

Modules could be depending on a certain state, before triggering or
subscribing to an event, this is not really possible to know without doing
static code analysis, so this is out of scope for this tool.

With this, we can detect possible contracts between different modules, ensuring
we know that testing between them is needed.


\subsection{End-To-End testing}

The final step in the testing pipeline, is to test the entire application
together. This is known as \gls*{e2e}. \gls*{e2e} is expensive, compared to the
other steps, as we have to load the entire application in the pipeline, and test
all interactions. This, of course, is the easiet way to cover all edge-cases, but
since it is the whole application being tested, harder to figure out what caused
a failure. Our \gls*{ide} can be saturated with events in the \gls*{e2e} step of a
pipeline, as all user interactions are translated into events, this ensures a
module developer can narrow down what modules are at fault, by what modules
\textit{subscribe} to that event.

\section{Modules} \label{sec:modules}

\paragraph{Event type} The event type allows for modules to pattern match on
specific events, and unlike, as in the previous version, modules can
\textit{subscribe} to specific events to react to. This changes the structure of
the module architecture to go from one wherein the core is a terminal object, to
a more \textit{complicated} one, in which module families can form.

This forms our \textit{request} and \textit{response} type, making the
equivalence with a \gls*{rest} \gls*{api} obvious. The clients and consumers
of the \gls*{api}, are the modules which can be seen in listing \ref{lst:mod}.

\begin{center}
  \lstinputlisting
    [ language=Rust
    , caption={Module trait (Rust)}
    , label=lst:mod
    , firstline=18
    , lastline=23
    ]{./libs/rust/core-module-lib/src/lib.rs}
\end{center}

A module can interact with the core, by getting the state, \gls*{ui},
\textit{throwing} an event, registration themselves to \textit{handle} an
event, or to \textit{send} a \textbf{CoreModification}. In listing
\ref{lst:core}, we can see this core trait.

\begin{code}[H]
  \lstinputlisting
    [ language=Rust
    , caption={Core trait (Rust)}
    , label=lst:core
    , firstline=6
    , lastline=18
    ]{./libs/rust/core-std-lib/src/core.rs}
\end{code}

Since a module updates the core by \textit{choosing} to send a
\textbf{CoreModification}, through a \gls*{mpsc}-channel, a module can run an
expensive computation on another thread, while \textit{ending} their
invocation, ensuring a smooth \gls*{ide} experience.


\subsection{Magnolia dependency graph visualizer}

In Magnolia, as in many other languages, one cannot have a cyclic dependency.
This means that the dependency graph of a Magnolia project should be a
\gls*{dag}. And since Magnolia has such a focus on reuse, the dependency graphs
in a Magnolia project could be quite large. Which means the cycles could be
quite long, which would make resolving the cyclic dependency issue complicated.
One way to help a developer, would be to give them a tool to visualize the
dependency graph, so that they could see what modules are connected. Using the
Magnolia library as the input, we can create a visualization of the dependencies
in Magnolia. Using two modules, one for \textit{parsing} the Magnolia library,
finding all packages, and their dependencies, and another for visualizing
this.

\begin{code}[H]
  \lstinputlisting
    [ language=Rust
    , caption={
      Magnolia library parser module \textit{subscribing} to an
      \textit{get\_magnolia\_graph } event (Rust)
    }
    , label=lst:magnoliaLibParserSub
    , firstline=33
    , lastline=36
    ]{./modules/magnolia\_dependency/src/lib.rs}
\end{code}

In listing \ref{lst:magnoliaLibParserSub}, the module is invoking the
\textit{add\_handler} method on an object that implements the \textit{Core} trait,
(\ref{lst:core}), and passing \textit{get\_magnolia\_graph} and
\textit{MODULE\_NAME}. This means that events with the event name
\textit{get\_magnolia\_graph}, will trigger this module. It's \textit{this}
module, because we have to pass the name of the module handling the event.

We can therefore invoke this module by simply triggering the subscribed event.

Due to Rust's type safety, there is a lot of \textit{noise}, especially because
we are working with recursive data structures, with optional values. In listing
\ref{lst:magLibParserSimple}, we can see a simplified version of the module
handler, but this is not valid Rust code.

\begin{code}[H]
  \lstinputlisting
    [ language=Rust
    , caption={
      Simplified magnolia library parser module. Note, this code will compile
      (Rust)
    }
    , label=lst:magLibParserSimple
    , numbers=left
    , numberstyle=\tiny\color{gray}
    ]{./code/mag\_lib\_simple.rs}
\end{code}

The code in listing \ref{lst:magLibParserSimple}, we are handling events with
the name \textit{get\_magnolia\_graph}, (line 3), and getting the path that is
supplied in the event argument, (line 4). In line 4 we then create a
\textit{key}, which we use to check if this graph has been created yet, by
checking the state (line 6). If this graph does exist, we \textit{respond} by
throwing an event with the existing graph (line 7 to 9). If it does not exist,
we create it by calling the \textit{get\_graph} function,
(left out for brevity), which recursively finds files in the supplied path,
using RegEx to find the packages, and their dependencies. We then end by
\textit{responding} with the created graph, (line 12), and store it in the
state, with the key (line 13 to 16). The resulting response can be seen in
listing \ref{lst:magLibParserRes}

\begin{code}[H]
  \begin{lstlisting}[
      language=TypeScript
    , label=lst:magLibParserRes
    , caption={Magnolia library parser response (TypeScript)}]
    {
      event: {
        event: "graph",
        args: {
          list: [
            {
              obj {
                name: { str: string },
                dependencies: { list: [{ str: string }] }
              }
            }
          ]
        }
      }
    }
  \end{lstlisting}
\end{code}

The module responsible for rendering the graph, uses D3, a visualization library
for JavaScript. D3 expects \textit{nodes} and \textit{links}, specified in
listing \ref{lst:D3Type}.

\begin{lstlisting}[
    language=TypeScript
  , label=lst:D3Type
  , caption={D3 expected input (TypeScript)}]
  type Node = { id: string, name: string };
  type Link = { source: string, target: string };
\end{lstlisting}

But due to how types are encoded in our Value type, as seen in
\ref{lst:magLibParserRes}, some translation is necessary. We have to go from the
type \textit{Value}, to
\textit{
  list of objects, with two fields, name and dependencies, of type string and
  list of string, respectively}
This translation can be seen in \ref{lst:depVisMod}.

Left out, are the steps verifying that the event has an argument,
(since its optional to pass one), and that the argument is of the list variant
of \textit{Value}. Since the list variant can contain any \textit{Value}
variant, we filter the list by whether it is an object variant, (line two), we
then transform each element in the list, into an intersection of the types D3
expects, (\ref{lst:D3Type}). The question mark syntax on line four, where we
declare the \textit{id} variable, means that if any of the expressions on the
left are undefined, the resulting expression is undefined. Since the object
variant of \textit{Value} does not necessarily contain the \textit{name} field,
or is of the kind \textit{str}.

On line nine to thirteen, we are getting all the dependencies from the object,
with a helper method, \textit{tObjLookUpOr}. This method does a \textit{lookup}
on the supplied field on an object, and a type check. If the object does not
exist, or is not of the correct type, the passed fallback value is returned
instead, in this case, an empty list (line ten). Since we know the value is a
list, we can safely access it, (line eleven), and filter by the string variant,
and transforming the \textit{Value} to a string primitive, (line eleven to thirteen).

\begin{code}[H]
  \lstinputlisting
    [ language=TypeScript
    , caption={
      Snippet from the dependency visualizer module, showing how we translate
      from our built-in representation of values, to the one expected by the D3
      library (TypeScript).
    }
    , label=lst:depVisMod
    , numbers=left
    , numberstyle=\tiny\color{gray}
    , firstline=203
    , lastline=217
    ]{./modules/dependency-viewer/module.ts}
\end{code}


\subsubsection{Rust dependency graph visualizer}

Using the same concept, we also developed a module for visualizing dependencies
in a Rust project. Similar to the Magnolia one, it is quite primitive, as it
simply uses RegEx to find imports, instead of a dedicated parser. As such, it
is prone to mistakes.

In picture \ref{pic:rustDeps}, we can see this projects dependencies visualized.

\begin{figure}
  \centering
    \includegraphics[width=1.0\textwidth]{rust-deps.png}
  \caption{
    Module that visualizes the dependencies in this project. As one can see, it
    contains several mistakes, like the \textbf{main} file depends on
    \textbf{app}, but there is no link between these two nodes. This is due to
    Rusts varied import statements, making it hard to capture with \textit{just}
    RegEx.
  }
  \label{pic:rustDeps}
\end{figure}

Since we are using RegEx, this primitive parser module cannot see that certain
files are part of any project, as many of the nodes without dependencies are
code examples used in the writing of this thesis.


\subsection{File System Abstraction (FSA)}

Many of the features needed in an \gls*{ide}, and in the modules showcased thus
far, are dependent on file system operations. The module which expose such
functionality to other modules, is called \textit{ide\_fsa}. It is written in
Rust, as Rust abstracts away the differences between \gls*{os} paths. As the
same path on a Windows system is represented differently on an unix system,
since the path separator is $\\$ and $/$ respectively. The following are a list
of file \gls*{os} system operations that are made available to modules, with the
help of the ide\_fsa module.

\begin{itemize}
  \item Open a file
  \item Create a file/folder
  \item Read from a file
  \item Move a file/folder
  \item Copy a file/folder
  \item Delete a file/folder
  \item List the contents of a folder
\end{itemize}

As shown in the picture \ref{pic:modDep}, from chapter \ref{cha:ide}, many of
the modules developed are dependent on ide\_fsa. This was a deliberate
choice, as not only would it mean we could rely on Rusts \gls*{os} abstractions,
ensuring the \gls*{ide} would work on different \gls*{os}es, but it also meant
we could delegate error handling from files to this one module. As doing such
file system operations\footnote{Also called Input/Output (IO)}, are inherently
error-prone. This is encoded by Rust, into a \textit{Result} type, which we
utilize to send Events, but to different modules depending on the result
variant.

Ide\_fsa listens for the following events:

\begin{itemize}
  \item fsa-read
  \item fsa-write
  \item fsa-dir
\end{itemize}

Which, depending on what argument the consuming module passed, maps to a
specific \gls*{io} operation, which may fail. We have decided to not handle the
failure on the consumer side, rather the provider side. So, if a user attempts
to open a file which does not exist, it will simply fail, and a notification
will appear. We decided for this approach, as it allows for simpler \gls*{ui}
focused modules. In a fully fledged implementation of the prototype modules
showcased, proper, expected, \gls*{ui} behavior should be implemented, like
showing explicitly to the user that the action they just did, clicking open
file, caused an error.

\begin{code}[H]
  \lstinputlisting
    [ language=Rust
    , firstline=46
    , lastline=67
    , label=lst:fsaRead
    , numbers=left
    , numberstyle=\tiny\color{gray}
    , caption={
      Snippet from the ide\_fsa module, showing how it handles events. If any of
      the functions from lines $3$ to $5$ return an error then we create an event,
      sending the error to the ide\_error module.
    }
    ]{./modules/ide\_fsa/src/lib.rs}
\end{code}

Looking at the fsa-read event, we can see in listing \ref{lst:fsaRead}, that the
events are matched by their event name, and mapped to the corresponding function.
In the case of fsa-read, it expects there to be a path argument, which we use to
read the contents from the file, if it exists, before \textit{responding} to the
consumer by sending an event with \textit{fsa-read-module}, where
\textit{module} corresponds to the module name passed in the original event
argument. This happens if there occurs no errors\footnotemark{} during the IO
operation. If it does fail, say because the file does not exist, then an event
is sent to the ide\_error module, displaying the error to the user.

\footnotetext{
  It is an error when the method \textbf{is\_ok}, on line nine, returns false
}


\subsection{File explorer}

In section \ref{sec:ide}, we covered some of the modules we have implemented for
our zero-core modular \gls*{ide}, amongst them \textit{ide\_explorer}. This
module is responsible for the file explorer view in our \gls*{ide}. This module
is of interest, since we have discussed how \textit{pureness} is an aspect we
want in our modules, meaning modifications of the \gls*{ide}, should happen
through the \gls*{ide}.

In the case of the ide\_explorer, we have explicitly broken this \textit{rule}.
In listing \ref{lst:expUnPure}, we can see how we on line ten, we invoke
a JavaScript function, the implementation of which, can be seen in listing
\ref{lst:unPure}.

\begin{code}[H]
  \lstinputlisting
    [ language=TypeScript
    , caption={
      Ide\_explorer module implementation, note the invocation of \textbf{run},
      on line ten, an function which is \textit{unpure}. (TypeScript)
    }
    , label=lst:expUnPure
    , numbers=left
    , numberstyle=\tiny\color{gray}
    , firstline=4
    ]{./modules/ide\_explorer/index.ts}
\end{code}

\begin{code}[H]
  \lstinputlisting
    [ language=JavaScript
    , caption={
      JavaScript code that is \textit{unpure}, meaning it directly modifies the
      \gls*{dom} outside of the \gls*{ide}. Used by the ide\_explorer module.
    }
    , label=lst:unPure
    , numbers=left
    , numberstyle=\tiny\color{gray}
    ]{./modules/ide\_explorer/unPureCode.js}
\end{code}

On lines four, six, fifteen, seventeen, and twenty, we are doing various direct
\gls*{dom} modifications. Of course, this is quite possible to do with in our
provided \gls*{api}, but it is still useful to showcase.

If, for some reason, future module developers implement a similar, unpure,
solution it will work, but other modules that remain \textit{pure} cannot
know the \textit{true} state of the \gls*{ide}, which should be taken into
consideration.


\section{Module installation} \label{sec:module-installation}

Manually installing compile-time modules can be tedious, so we automated the
task. We are using the term \textit{installation} to mean adding the module to
the \gls*{ide}. This can be as simple as adding a script tag to the \gls*{dom},
with the source being a JavaScript file, or more complex, loading a library
which has the correct bindings to be a Rust runtime module.

For \gls*{jsms}, it is quite trivial, simply bundle the JavaScript code into a
single script, and import it into one file, and, as a final step, bundle this
file. This ensures that all JavaScript modules are included, and loaded after
the core system.

For the \gls*{rsms}, it is less trivial. A compile-time Rust module is a Rust
crate, and since we are using Cargo\footnote{Build tool for Rust} we need to
specify all our imported crates in a \textit{Cargo.toml} file. We then also
need to explicitly import and add the module into a list of all compile-time
modules. With a build script, we have automated this. In the figure
\ref{fig:compMod}, we can see a diagram of this process.

\begin{figure}
  \centering
  \input{./figures/compile-time-module}
  \caption{
    Diagram of compile time module integration, we can see that when we invoke
    the installer tool, it reads the Modules.toml config, gets the necessary
    module paths, and \textit{installs} them by writing them onto a file,
    \textit{module\_reg.rs}, which is imported by the \gls*{ide}. All imports
    need to be specified in \textit{Cargo.toml}, which is also handled by the
    installer tool.
  }
  \label{fig:compMod}
\end{figure}

For runtime modules, the installation is divided into two steps. First, we have
to add the source file to the application directory\footnote{Located in \textit{\%APPDATA} on Windows, or \textit{~/.share} on Unix},
before actually \textit{installing} the module. Depending on what kind of file
it is, this installation process can vary.


\subsection{Rust runtime module installation}

We use the term runtime here, to strictly mean post compilation. When this
\gls*{ide} is running in user-space, we cannot use newly installed Rust
runtime modules. This is because the \gls*{ide} only looks for Rust runtime
modules during startup, so a restart is necessary. Given that a Rust runtime
module is a simple library, we know what functions it exposes, and the calling
conventions. Along with the abi\_stable crate, we can ensure no undefined
behavior happens during runtime.


\subsection{JavaScript runtime module installation}

Unlike the Rust runtime modules, JavaScript runtime modules can be installed and
used post-startup. This is because there is no difference on how a JavaScript
module is installed, regardless if the installation happens during compile time
or runtime. Since all a JavaScript module is, is a script element on the
\gls*{dom}, we can add new ones whenever. The only thing that differentiates the
two, is that for runtime modules, we have to call the \textbf{init} method after
installing it, instead of during compile-time, when all modules are invoked
after every module has been installed.

